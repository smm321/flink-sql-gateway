  <configuration  xmlns:xi="http://www.w3.org/2001/XInclude">
    
    <property>
      <name>fs.defaultFS</name>
      <value>hdfs://idstaging1</value>
    </property>
    
    <!-- <property>
      <name>fs.defaultFS.hdfs_dev_client</name>
      <value>viewfs://dev</value>
    </property>
    
    <property>
      <name>fs.defaultFS.hdfs_staging_client</name>
      <value>viewfs://staging</value>
    </property>
    
    <property>
      <name>fs.defaultFS.hdfs_test_client</name>
      <value>viewfs://test</value>
    </property>
    
    <property>
      <name>fs.defaultFS.hdfs_uat_client</name>
      <value>viewfs://uat</value>
    </property> -->
    
    <property>
      <name>fs.protected.directories</name>
      <value>/user,/projects,/tmp,/logs,/logs/yarn,/logs/spark,/packages,/packages/jars,/yarn,/mapred,/history</value>
    </property>
    
    <property>
      <name>fs.trash.interval</name>
      <value>1440</value>
    </property>

    <property>
      <name>fs.trash.checkpoint.interval</name>
      <value>180</value>
    </property>
   
    <property>
      <name>fs.trash.root</name>
      <value>/Trash/${user.name}</value>
    </property>
    
    <property>
      <name>ha.health-monitor.check-interval.ms</name>
      <value>2000</value>
    </property>
    
    <property>
      <name>ha.health-monitor.rpc-timeout.ms</name>
      <value>180000</value>
    </property>
    
    <property>
      <name>ha.zookeeper.quorum</name>
      <value>id-staging-hadoop01-10-162-34-126:2181,id-staging-hadoop02-10-162-34-125:2181,id-staging-hadoop03-10-162-34-124:2181</value>
    </property>
    
    <property>
      <name>hadoop.http.staticuser.user</name>
      <value>work</value>
    </property>
    
    <property>
      <name>hadoop.proxyuser.*</name>
      <value>*</value>
    </property>
    
    <property>
      <name>hadoop.proxyuser.dlm.groups</name>
      <value>*</value>
    </property>

    <property>
      <name>hadoop.proxyuser.dlm.hosts</name>
      <value>*</value>
    </property>

    <property>
      <name>hadoop.proxyuser.clickhouse.groups</name>
      <value>*</value>
    </property>
    
    <property>
      <name>hadoop.proxyuser.clickhouse.hosts</name>
      <value>*</value>
    </property>
    
    <property>
      <name>hadoop.proxyuser.flink.groups</name>
      <value>*</value>
    </property>
    
    <property>
      <name>hadoop.proxyuser.flink.hosts</name>
      <value>*</value>
    </property>
    
    <property>
      <name>hadoop.proxyuser.genie.groups</name>
      <value>*</value>
    </property>
    
    <property>
      <name>hadoop.proxyuser.genie.hosts</name>
      <value>*</value>
    </property>
    
    <property>
      <name>hadoop.proxyuser.hive.groups</name>
      <value>*</value>
    </property>
    
    <property>
      <name>hadoop.proxyuser.hive.hosts</name>
      <value>*</value>
    </property>
    
    <property>
      <name>hadoop.proxyuser.presto.groups</name>
      <value>*</value>
    </property>
    
    <property>
      <name>hadoop.proxyuser.presto.hosts</name>
      <value>*</value>
    </property>

    <property>
      <name>hadoop.proxyuser.hue.groups</name>
      <value>*</value>
    </property>

    <property>
      <name>hadoop.proxyuser.hue.hosts</name>
      <value>*</value>
    </property>
    
    <property>
      <name>hadoop.proxyuser.work.groups</name>
      <value>*</value>
    </property>
    
    <property>
      <name>hadoop.proxyuser.work.hosts</name>
      <value>*</value>
    </property>
    
    <property>
      <name>hadoop.security.auth_to_local</name>
      <value></value>
    </property>
    
    <property>
      <name>hadoop.security.authentication</name>
      <value>simple</value>
    </property>
    
    <property>
      <name>hadoop.security.authorization</name>
      <value>false</value>
    </property>
   
    <property>
      <name>hadoop.security.group.mapping</name>
      <value>org.apache.hadoop.security.ShellBasedUnixGroupsMapping</value>
    </property>
    
    <property>
      <name>hadoop.tmp.dir</name>
      <value>/tmp/${user.name}</value>
    </property>
    
    <property>
      <name>hadoop.zk.address</name>
      <value>id-staging-hadoop01-10-162-34-126:2181,id-staging-hadoop02-10-162-34-125:2181,id-staging-hadoop03-10-162-34-124:2181</value>
    </property>
  <!--  
    <property>
      <name>io.compression.codec.lzo.class</name>
      <value>com.hadoop.compression.lzo.LzoCodec</value>
    </property>
    
    <property>
      <name>io.compression.codecs</name>
      <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec</value>
    </property>
    -->
    <property>
      <name>io.file.buffer.size</name>
      <value>131072</value>
    </property>
    
    <property>
      <name>io.serializations</name>
      <value>org.apache.hadoop.io.serializer.WritableSerialization</value>
    </property>
    
    <property>
      <name>ipc.8020.callqueue.impl</name>
      <value>org.apache.hadoop.ipc.FairCallQueue</value>
    </property>
    
    <property>
      <name>ipc.8020.cost-provider.impl</name>
      <value>org.apache.hadoop.ipc.WeightedTimeCostProvider</value>
    </property>
    
    <property>
      <name>ipc.8020.decay-scheduler.static.user.queue.1</name>
      <value>presto</value>
    </property>
    
    <property>
      <name>ipc.8020.faircallqueue.multiplexer.weights</name>
      <value>16,8,4,1</value>
    </property>
    
    <property>
      <name>ipc.8020.scheduler.impl</name>
      <value>org.apache.hadoop.ipc.DecayRpcScheduler</value>
    </property>
    
    <property>
      <name>ipc.8888.callqueue.impl</name>
      <value>org.apache.hadoop.ipc.FairCallQueue</value>
    </property>    
    
    <property>
      <name>ipc.8888.cost-provider.impl</name>
      <value>org.apache.hadoop.ipc.WeightedTimeCostProvider</value>
    </property>    

    <property>
      <name>ipc.8888.decay-scheduler.static.user.queue.1</name>
      <value>presto</value>
    </property>    
  
    <property>
      <name>ipc.8888.faircallqueue.multiplexer.weights</name>
      <value>16,8,4,1</value>
    </property>

    <property>
      <name>ipc.8888.scheduler.impl</name>
      <value>org.apache.hadoop.ipc.DecayRpcScheduler</value>
    </property>

    <property>
      <name>ipc.8888.backoff.enable</name>
      <value>true</value>
    </property>
    
    <property>
      <name>ipc.client.connect.max.retries</name>
      <value>50</value>
    </property>
    
    <property>
      <name>ipc.client.connect.timeout</name>
      <value>90000</value>
    </property>
    
    <property>
      <name>ipc.client.connection.maxidletime</name>
      <value>30000</value>
    </property>
    
    <property>
      <name>ipc.client.idlethreshold</name>
      <value>8000</value>
    </property>
    
    <property>
      <name>ipc.server.listen.queue.size</name>
      <value>10240</value>
      <final>true</final>
    </property>
    
    <property>
      <name>ipc.server.read.threadpool.size</name>
      <value>10</value>
    </property>
 <!-- 
    <property>
      <name>net.topology.script.file.name</name>
      <value>/etc/hadoop/topology_script.py</value>
    </property> -->
    
    <property>
      <name>hadoop.caller.context.enabled</name>
      <value>true</value>
    </property>

    <property>
      <name>hadoop.security.rpc-password.shadow.file</name>
      <value>/etc/hadoop-server/shadow</value>
    </property>
    
    <property>
      <name>hadoop.security.rpc-password.shadow.file.checksum.enabled</name>
      <value>true</value>
    </property> 
  </configuration>
